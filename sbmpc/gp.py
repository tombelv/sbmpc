import jax
import jax.numpy as jnp
import gpjax as gpx
import numpy as np
import pandas as pd
import time
import httpimport
import numpyro
import numpyro.distributions as dist
import os

from numpyro.infer import MCMC, NUTS, Predictive
from numpyro import handlers
from sbmpc import hmc
from sklearn.metrics import mean_squared_error, r2_score # TODO - check these after optimisation
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from jax.scipy.stats import norm

from functools import partial
# with httpimport.github_repo('martin-marek', 'mini-hmc-jax', ref='master'):
#   import hmc

from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, WhiteKernel, DotProduct

MASS = 0.027
GRAVITY = 9.81
INPUT_HOVER = jnp.array([MASS*GRAVITY, 0., 0., 0.], dtype=jnp.float32)

num_steps = 50
num_samples = 10
sim_iters = 500
horizon = 25
              

class GaussianProcessSampling(): 
    def __init__(self):
        self.key = jax.random.key(456)
        self.n_samples = 500  
        # self.n_samples = 300
        self.P = np.ones((self.n_samples,1,1)) 
        self.training_set = None
        self.test_set = None
        self.delta = 0.5 # or think in terms of  1 - delta -> see paper (constraint violation between 0 and n_obstacles)
        self.burn_in = 10

        total_samples = num_steps * num_samples
        all_samples = pd.read_csv("/home/ubuntu/sbmpc/sbmpc/datasets/dataset_2.data", header=None, delimiter=' ').values[:total_samples] # get dataset generated by dataset.py
        print(f"Retrieved dataset of {all_samples.shape[0]} samples with {all_samples.shape[1]} dimensions") # ((num_samples * num_steps), 114)

        X = all_samples[:, :-1]  
        y = all_samples[:, -1].reshape(-1, 1)

        Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.3, random_state=42) # split into training and test set

        self.Xtr = Xtr
        self.ytr = ytr
        self.Xte = Xte
        self.yte = yte

        log_ytr = np.log(ytr)
        log_yte = np.log(yte)

        y_scaler = StandardScaler().fit(log_ytr)
        scaled_ytr = y_scaler.transform(log_ytr)
        scaled_yte = y_scaler.transform(log_yte)

        x_scaler = StandardScaler().fit(Xtr)
        scaled_Xtr = x_scaler.transform(Xtr)
        scaled_Xte = x_scaler.transform(Xte)

        # n_train = Xtr.shape
        n_train, n_covariates = scaled_Xtr.shape
        kernel = gpx.kernels.RBF(             
            active_dims=list(range(n_covariates)),
            variance=jnp.var(scaled_ytr),
            lengthscale=0.1 * jnp.ones((n_covariates,)),
            )
        likelihood = gpx.likelihoods.Gaussian(num_datapoints=n_train) 
        mean = gpx.mean_functions.Zero()  

        prior = gpx.gps.Prior(mean_function = mean, kernel = kernel) 
        posterior = prior * likelihood

        self.training_set = gpx.Dataset(X=scaled_Xtr, y=scaled_ytr)  
        self.test_set = gpx.Dataset(X=scaled_Xte, y=scaled_yte)

        print("Optimising ...")
        self.opt_posterior, history = gpx.fit_scipy( # TODO - find a way to store so actually offline. Also try fit instead of fit_scipy
        model=posterior,
        objective=lambda p, d: -gpx.objectives.conjugate_loocv(p,d),   # vs conjugate loocv and others
        train_data=self.training_set,
        ) 

    @partial(jax.jit, static_argnums=(0,))
    def get_P(self, X):
        latent_dist = self.opt_posterior.predict(X, train_data=self.training_set) # get GP prediction for X
        mean = latent_dist.mean()
        stddev = latent_dist.stddev()
        
        P = norm.cdf(self.delta, loc=mean, scale=stddev) # calc cdf for samples based on predictive mean and stddev, below a set threshold
        return np.reshape(P,(self.n_samples,1,1))    
   

    @partial(jax.jit, static_argnums=(0,))
    def ravel_gp(self, X, state):
        flat_samples = jnp.tile(X, (1,5,1))  # tile control points  - check that this is correct
        x,y,z = flat_samples.shape
        flat_samples = jnp.reshape(flat_samples, (x, y*z))   # flatten

        state = jnp.tile(state,(x,1)) # add state for prediction
        flat_samples = jnp.concatenate([state,flat_samples], axis=1)

        return flat_samples


    @partial(jax.jit, static_argnums=(0,)) 
    def sample(self, X,state): 
        flat_X = self.ravel_gp(X, state) # transform and add state for prediction

        predictive_dist = self.get_P(flat_X) # get GP-based CDF 

        target = X * predictive_dist # multiply distributions to get target
        mean = jnp.mean(target)
        stddev = jnp.std(target)

        key = jax.random.key(456)
        params_init = jnp.zeros((5,4))  # initialise control vars - TODO replace with better initial guess

        @partial(jax.jit, static_argnums=(0,))
        def target_pdf(params):
            return jax.scipy.stats.multivariate_normal.logpdf(x=params, mean=mean, cov=stddev).sum()

            
        # sample from target with hmc
        chain = hmc.sample(key, params_init, target_pdf, n_steps = (self.burn_in + self.n_samples), n_leapfrog_steps=100, step_size=0.1) # try different lengths of chains - consider burn-in

        return chain
    

class BNNSampling():
    def __init__(self):
        total_samples = num_steps * 20
        all_samples = pd.read_csv("/home/ubuntu/sbmpc/sbmpc/datasets/dataset_2.data", header=None, delimiter=' ').values[:total_samples] # get dataset generated by dataset.py
        print(f"Retrieved dataset of {all_samples.shape[0]} samples with {all_samples.shape[1]} dimensions") # ((num_samples * num_steps), 114)

        X = all_samples[:, :-1]  
        y = all_samples[:, -1].reshape(-1, 1)

        Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.3, random_state=42) # split into training and test set

        self.Xtr = Xtr
        self.ytr = ytr
        self.Xte = Xte
        self.yte = yte

        log_ytr = np.log(ytr)
        log_yte = np.log(yte)

        y_scaler = StandardScaler().fit(log_ytr)
        scaled_ytr = y_scaler.transform(log_ytr)
        scaled_yte = y_scaler.transform(log_yte)

        x_scaler = StandardScaler().fit(Xtr)
        scaled_Xtr = x_scaler.transform(Xtr)
        scaled_Xte = x_scaler.transform(Xte)

        self.delta = 2.5
        self.n_samples = 500
        self.burn_in = 10

        def model(X, Y= None):
            w1 = numpyro.sample("w1", dist.Normal(0, 1).expand([X.shape[1], 10])) # TODO - look into more/better layers
            b1 = numpyro.sample("b1", dist.Normal(0, 1).expand([10]))

            w2 = numpyro.sample("w2", dist.Normal(0, 1).expand([10, 1]))
            b2 = numpyro.sample("b2", dist.Normal(0, 1))

            hidden = jnp.tanh(jnp.dot(X, w1) + b1)
            mean = jnp.dot(hidden, w2) + b2

            sigma = numpyro.sample("sigma", dist.Exponential(1.0))
            numpyro.sample("Y", dist.Normal(mean, sigma), obs=y)

        kernel = NUTS(model)
        mcmc = MCMC(kernel, num_warmup=100, num_samples=500)
        mcmc.run(jax.random.PRNGKey(0), X=X, Y=ytr)

        self.posterior_samples = mcmc.get_samples()
        self.pred_dist = Predictive(model, posterior_samples=self.posterior_samples,return_sites=["Y"]) 


    @partial(jax.jit, static_argnums=(0,))
    def get_P(self, X):
        predictions = self.pred_dist(rng_key=jax.random.PRNGKey(1), X=X)["Y"] 
        latent_dist = jnp.squeeze(predictions)

        mean = jnp.mean(latent_dist,axis=1)
        stddev = jnp.mean(latent_dist,axis=1)

        P = norm.cdf(self.delta, loc=mean, scale=stddev) # calc cdf for samples based on predictive mean and stddev, below a set threshold
        
        return np.reshape(P,(self.n_samples,1,1))
        

    @partial(jax.jit, static_argnums=(0,))
    def ravel_bnn(self, X, state):
        flat_samples = jnp.tile(X, (1,5,1))  # tile control points  - check that this is correct
        x,y,z = flat_samples.shape
        flat_samples = jnp.reshape(flat_samples, (x, y*z))   # flatten

        state = jnp.tile(state,(x,1)) # add state for prediction
        flat_samples = jnp.concatenate([state,flat_samples], axis=1)

        return flat_samples

    @partial(jax.jit, static_argnums=(0,))
    def sample(self, X, state):
        flat_X = self.ravel_bnn(X, state) # transform and add state for prediction

        predictive_dist = self.get_P(flat_X) # get GP-based CDF 

        target = X * predictive_dist # multiply distributions to get target
        
        mean = jnp.mean(target)  # too small
        stddev = jnp.std(target)

        key = jax.random.key(456)
        params_init = jnp.zeros((5,4))  # initialise control vars - TODO replace with better initial guess

        @partial(jax.jit, static_argnums=(0,))
        def target_pdf(params):
            return jax.scipy.stats.multivariate_normal.logpdf(x=params, mean=mean, cov=stddev).sum()

        # sample from target with hmc
        chain = hmc.sample(key, params_init, target_pdf, n_steps = (self.burn_in + self.n_samples), n_leapfrog_steps=100, step_size=0.1) # try different lengths of chains - consider burn-in
        return chain

    
if __name__ == "__main__":
    # gp = GaussianProcessSampling()  # reoptimise gp
    # gp.sk_gp()
    bnn = BNNSampling()
    # test = [0.000000, 0.000000, 0.000000, 1.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, -0.009388, -0.008598, -0.007890, -0.007343, -0.007040, -0.007062, -0.007489, -0.008379, -0.009689, -0.011356, -0.013312, -0.015493, -0.017833, -0.020242, -0.022537, -0.024508, -0.025946, -0.026643, -0.026390, -0.024979, -0.022274, -0.018429, -0.013674, -0.008234, -0.002339 ]
    # test = [0.025547, -0.016850, -0.018652, 0.999941, 0.008251, 0.007064, 0.000716, 0.071290, -0.057724, -0.032891, 0.035120, 0.016117, 0.000678, -0.028416, 0.016508, 0.026836, -0.013883, -0.026409, 0.008893, 0.023160, -0.011814, -0.024341, 0.001637, 0.019690, -0.009621, -0.022154, -0.004905, 0.016638, -0.007174, -0.019790, -0.010383, 0.014217, -0.004346, -0.017190, -0.014444, 0.012642, -0.001006, -0.014295, -0.016742, 0.012125, 0.002977, -0.011114, -0.017072, 0.012770, 0.007633, -0.007966, -0.015747, 0.014234, 0.012528, -0.005243, -0.013205, 0.016060, 0.017111, -0.003340, -0.009883, 0.017792, 0.020832, -0.002651, -0.006221, 0.018975, 0.023139, -0.003577, -0.002652, 0.019156, 0.023476, -0.006371, 0.000475, 0.018031, 0.021488, -0.010623, 0.003080, 0.015820, 0.017690, -0.015751, 0.005149, 0.012873, 0.012817, -0.021174, 0.006669, 0.009540, 0.007605, -0.026311, 0.007624, 0.006170, 0.002787, -0.030580, 0.008003, 0.003114, -0.000902, -0.033395, 0.007788, 0.000717, -0.002722, -0.034302, 0.006978, -0.000780, -0.002124, -0.033482, 0.005655, -0.001462, 0.000655, -0.031279, 0.003922, -0.001494, 0.005180, -0.028034, 0.001880, -0.001041, 0.011016, -0.023839, -0.000223, -0.000291, 0.017612]
    # print(gp.opt_posterior.predict(jnp.array(test), train_data=gp.training_set))
    # mock_X = jax.random.normal(key=jax.random.key(456), shape= (1999,113)) 

