import jax
import jax.numpy as jnp
import gpjax as gpx

import numpy as np
import pandas as pd
import time
import httpimport

from sbmpc import hmc
from sklearn.metrics import mean_squared_error, r2_score # TODO - check these after optimisation
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from jax.scipy.stats import norm

from functools import partial
# with httpimport.github_repo('martin-marek', 'mini-hmc-jax', ref='master'):
#   import hmc

MASS = 0.027
GRAVITY = 9.81
INPUT_HOVER = jnp.array([MASS*GRAVITY, 0., 0., 0.], dtype=jnp.float32)

num_steps = 50
num_samples = 10
sim_iters = 500
horizon = 25
              

class GaussianProcessSampling(): 

    def __init__(self):
        self.key = jax.random.key(456)
        self.n_samples = 1999
        self.P = np.ones((self.n_samples,1,1)) 
        self.training_set = None
        self.test_set = None
        self.delta = 1 # or think in terms of  1 - delta -> see paper (constraint violation between 0 and n_obstacles)

        total_samples = num_steps * num_samples
        all_samples = pd.read_csv("/home/ubuntu/sbmpc/sbmpc/datasets/dataset_2.data", header=None, delimiter=' ').values[:total_samples] # get dataset generated by dataset.py
        print(f"Retrieved dataset of {all_samples.shape[0]} samples with {all_samples.shape[1]} dimensions") # ((num_samples * num_steps), 114)

        X = all_samples[:, :-1]  
        y = all_samples[:, -1].reshape(-1, 1)

        Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.3, random_state=42) # split into training and test set

        log_ytr = np.log(ytr)
        log_yte = np.log(yte)

        y_scaler = StandardScaler().fit(log_ytr)
        scaled_ytr = y_scaler.transform(log_ytr)
        scaled_yte = y_scaler.transform(log_yte)

        x_scaler = StandardScaler().fit(Xtr)
        scaled_Xtr = x_scaler.transform(Xtr)
        scaled_Xte = x_scaler.transform(Xte)

        # n_train = Xtr.shape
        n_train, n_covariates = scaled_Xtr.shape
        kernel = gpx.kernels.RBF(             
            active_dims=list(range(n_covariates)),
            variance=jnp.var(scaled_ytr),
            lengthscale=0.1 * jnp.ones((n_covariates,)),
            )
        likelihood = gpx.likelihoods.Gaussian(num_datapoints=n_train) 
        mean = gpx.mean_functions.Zero()  

        prior = gpx.gps.Prior(mean_function = mean, kernel = kernel) 
        posterior = prior * likelihood

        self.training_set = gpx.Dataset(X=scaled_Xtr, y=scaled_ytr)  
        self.test_set = gpx.Dataset(X=scaled_Xte, y=scaled_yte)

        print("Optimising ...")
        self.opt_posterior, history = gpx.fit_scipy(
        model=posterior,
        objective=lambda p, d: -gpx.objectives.conjugate_loocv(p,d),   # vs conjugate loocv and others
        train_data=self.training_set,
        ) 

    @partial(jax.jit, static_argnums=(0,))
    def get_P(self, X):
        latent_dist = self.opt_posterior.predict(X, train_data=self.training_set) # get GP prediction for X
        mean = latent_dist.mean()
        stddev = latent_dist.stddev()

        P = norm.cdf(self.delta, loc=mean, scale=stddev) # calc cdf for samples based on predictive mean and stddev, below a set threshold
  
        return np.reshape(P,(self.n_samples,1,1))    
   
    @partial(jax.jit, static_argnums=(0,)) 
    def get_target_dist(self, X): 
        P = self.get_P(X)   # get probability of observing these samples based on the gp
        target_dist = X * P

        return target_dist

    @partial(jax.jit, static_argnums=(0,)) 
    def hmc_sampling(self, X):
        target = self.get_target_dist(X)  # define target distribution
        mean = jnp.mean(target)
        stddev = jnp.std(target)

        key = jax.random.key(456)
        params_init = jnp.zeros((25,4))  # initialise control vars - TODO replace with better inital guess

        @partial(jax.jit, static_argnums=(0,))
        def target_pdf(params):
            return jax.scipy.stats.multivariate_normal.logpdf(x=params, mean=mean, cov=stddev).sum()
       
        chain = hmc.sample(key, params_init, target_pdf, n_steps=100, n_leapfrog_steps=100, step_size=0.1) # try different lengths of chains - consider burn-in

        return chain
    

    
if __name__ == "__main__":
    gp = GaussianProcessSampling()  # reoptimise gp

    # test = [0.000000, 0.000000, 0.000000, 1.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, -0.009388, -0.008598, -0.007890, -0.007343, -0.007040, -0.007062, -0.007489, -0.008379, -0.009689, -0.011356, -0.013312, -0.015493, -0.017833, -0.020242, -0.022537, -0.024508, -0.025946, -0.026643, -0.026390, -0.024979, -0.022274, -0.018429, -0.013674, -0.008234, -0.002339 ]
    # test = [0.025547, -0.016850, -0.018652, 0.999941, 0.008251, 0.007064, 0.000716, 0.071290, -0.057724, -0.032891, 0.035120, 0.016117, 0.000678, -0.028416, 0.016508, 0.026836, -0.013883, -0.026409, 0.008893, 0.023160, -0.011814, -0.024341, 0.001637, 0.019690, -0.009621, -0.022154, -0.004905, 0.016638, -0.007174, -0.019790, -0.010383, 0.014217, -0.004346, -0.017190, -0.014444, 0.012642, -0.001006, -0.014295, -0.016742, 0.012125, 0.002977, -0.011114, -0.017072, 0.012770, 0.007633, -0.007966, -0.015747, 0.014234, 0.012528, -0.005243, -0.013205, 0.016060, 0.017111, -0.003340, -0.009883, 0.017792, 0.020832, -0.002651, -0.006221, 0.018975, 0.023139, -0.003577, -0.002652, 0.019156, 0.023476, -0.006371, 0.000475, 0.018031, 0.021488, -0.010623, 0.003080, 0.015820, 0.017690, -0.015751, 0.005149, 0.012873, 0.012817, -0.021174, 0.006669, 0.009540, 0.007605, -0.026311, 0.007624, 0.006170, 0.002787, -0.030580, 0.008003, 0.003114, -0.000902, -0.033395, 0.007788, 0.000717, -0.002722, -0.034302, 0.006978, -0.000780, -0.002124, -0.033482, 0.005655, -0.001462, 0.000655, -0.031279, 0.003922, -0.001494, 0.005180, -0.028034, 0.001880, -0.001041, 0.011016, -0.023839, -0.000223, -0.000291, 0.017612]
    # print(gp.opt_posterior.predict(jnp.array(test), train_data=gp.training_set))
    # mock_X = jax.random.normal(key=jax.random.key(456), shape= (1999,113)) 
