import jax
import jax.numpy as jnp
import gpjax as gpx
import numpy as np
import pandas as pd
import time
import httpimport
import numpyro
import numpyro.distributions as dist
import os

# import matplotlib as plt
import matplotlib.pyplot as plt
from numpyro.infer import MCMC, NUTS, Predictive
from numpyro import handlers
from sbmpc import hmc
from sklearn.metrics import mean_squared_error, r2_score # TODO - check these after optimisation
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from jax.scipy.stats import norm

from functools import partial
# with httpimport.github_repo('martin-marek', 'mini-hmc-jax', ref='master'):
#   import hmc

from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, WhiteKernel, DotProduct

MASS = 0.027
GRAVITY = 9.81
INPUT_HOVER = jnp.array([MASS*GRAVITY, 0., 0., 0.], dtype=jnp.float32)

num_steps = 50
num_samples = 10
sim_iters = 500
horizon = 25
              

class GaussianProcessSampling(): 
    def __init__(self, n_samples):
        self.key = jax.random.key(456)
        self.n_samples = n_samples
        self.P = np.ones((self.n_samples,1,1)) 
        self.training_set = None
        self.test_set = None
        self.delta = 0.3
        # self.delta = 0.5 
        self.burn_in = 10

        total_samples = num_steps * num_samples
        all_samples = pd.read_csv("/home/ubuntu/sbmpc/sbmpc/datasets/dataset_2.data", header=None, delimiter=' ').values[:total_samples] # get dataset generated by dataset.py
        print(f"Retrieved dataset of {all_samples.shape[0]} samples with {all_samples.shape[1]} dimensions") # ((num_samples * num_steps), 114)

        X = all_samples[:, :-1]  
        y = all_samples[:, -1].reshape(-1, 1)

        Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.3, random_state=42) # split into training and test set

        self.Xtr = Xtr
        self.ytr = ytr
        self.Xte = Xte
        self.yte = yte

        log_ytr = np.log(ytr)
        log_yte = np.log(yte)

        y_scaler = StandardScaler().fit(log_ytr)
        scaled_ytr = y_scaler.transform(log_ytr)
        scaled_yte = y_scaler.transform(log_yte)

        x_scaler = StandardScaler().fit(Xtr)
        scaled_Xtr = x_scaler.transform(Xtr)
        scaled_Xte = x_scaler.transform(Xte)

        # n_train = Xtr.shape
        n_train, n_covariates = scaled_Xtr.shape
        kernel = gpx.kernels.RBF(             
            active_dims=list(range(n_covariates)),
            variance=jnp.var(scaled_ytr),
            lengthscale=jnp.ones((n_covariates,)),
            )
        likelihood = gpx.likelihoods.Gaussian(num_datapoints=n_train) 
        # mean = gpx.mean_functions.Zero()  
        mean = gpx.mean_functions.Constant(jnp.mean(scaled_ytr))

        prior = gpx.gps.Prior(mean_function = mean, kernel = kernel) 
        posterior = prior * likelihood

        self.training_set = gpx.Dataset(X=scaled_Xtr, y=scaled_ytr)  
        self.test_set = gpx.Dataset(X=scaled_Xte, y=scaled_yte)

        print("Optimising ...")
        self.opt_posterior, history = gpx.fit_scipy( 
        model=posterior,
        objective=lambda p, d: -gpx.objectives.conjugate_mll(p,d),  
        train_data=self.training_set,
        ) 

        Xte = Xte
        yte = yte
        latent_dist = self.opt_posterior.predict(Xte, train_data=self.training_set)
        predictive_dist = self.opt_posterior.likelihood(latent_dist)

        predictive_mean = predictive_dist.mean()
        # print(predictive_mean)
        predictive_std = jnp.sqrt(predictive_dist.variance())

        feature_idx = 52
        sorted_idx = jnp.argsort(Xte[:, feature_idx])
        Xte_sorted = Xte[sorted_idx]
        mean_sorted = predictive_mean[sorted_idx]
        std_sorted = predictive_std[sorted_idx]
        if yte is not None:
            yte_sorted = yte[sorted_idx]


        cols = plt.rcParams["axes.prop_cycle"].by_key()["color"]
        fig, ax = plt.subplots(figsize=(8, 3), constrained_layout=True)

        ax.plot(X[:, feature_idx], y, "x", label="Correct Outputs", color="green", alpha= 1)

        # ax.fill_between(
        #     Xte_sorted[:, feature_idx],
        #     mean_sorted - 2 * std_sorted,
        #     mean_sorted + 2 * std_sorted,
        #     alpha=0.2,
        #     label="Two sigma",
        #     color=cols[1],
        # )
        # ax.plot(
        #     Xte_sorted[:, feature_idx],
        #     mean_sorted - 2 * std_sorted,
        #     linestyle="--",
        #     linewidth=1,
        #     color=cols[1],
        # )
        # ax.plot(
        #     Xte_sorted[:, feature_idx],
        #     mean_sorted + 2 * std_sorted,
        #     linestyle="--",
        #     linewidth=1,
        #     color=cols[1],
        # )

        if yte is not None:
            ax.plot(
                Xte_sorted[:, feature_idx],
                yte_sorted,
                label="Latent function",
                color="blue",
                linestyle="solid",
                linewidth=2,
            )

        # ax.plot(
        #     Xte_sorted[:, feature_idx],
        #     mean_sorted,
        #     label="Predictive mean",
        #     color=cols[1],
        # )

        # ax.legend(loc="center left")
        ax.legend()
        ax.set(xlabel=f"Input", ylabel="Output", title="GP latent distribution")
        plt.savefig("gp_plot.png")
        plt.show()


        # # Plotting
        # cols = plt.rcParams["axes.prop_cycle"].by_key()["color"]
        # latent_dist = self.opt_posterior.predict(Xte, train_data=self.training_set)
        # predictive_dist = self.opt_posterior.likelihood(latent_dist)

        # predictive_mean = predictive_dist.mean()
        # predictive_std = jnp.sqrt(predictive_dist.variance())

        # fig, ax = matplotlib.pyplot.subplots(figsize=(7.5, 2.5))
        # print(Xte.shape)
        # ax.plot(X, y, "x", label="Observations", color=cols[0], alpha=0.5)
        # ax.fill_between(
        #     Xte[:,0].squeeze(),
        #     predictive_mean - 2 * predictive_std,
        #     predictive_mean + 2 * predictive_std,
        #     alpha=0.2,
        #     label="Two sigma",
        #     color=cols[1],
        # )
        # ax.plot(
        #     Xte[:,0],
        #     predictive_mean - 2 * predictive_std,
        #     linestyle="--",
        #     linewidth=1,
        #     color=cols[1],
        # )
        # ax.plot(
        #     Xte[:,0],
        #     predictive_mean + 2 * predictive_std,
        #     linestyle="--",
        #     linewidth=1,
        #     color=cols[1],
        # )
        # ax.plot(
        #     Xte[:,0], yte, label="Latent function", color=cols[0], linestyle="--", linewidth=2
        # )
        # ax.plot(Xte[:,0], predictive_mean, label="Predictive mean", color=cols[1])
        # ax.legend(loc="center left", bbox_to_anchor=(0.975, 0.5))
        # matplotlib.pyplot.savefig("test.png")
        # matplotlib.pyplot.show()

    @partial(jax.jit, static_argnums=(0,))
    def get_P(self, X):
        latent_dist = self.opt_posterior.predict(X, train_data=self.training_set) # get GP prediction for X
        mean = latent_dist.mean()
        stddev = latent_dist.stddev()
        
        P = norm.cdf(self.delta, loc=mean, scale=stddev) # calc cdf for samples based on predictive mean and stddev, below a set threshold
        return np.reshape(P,(self.n_samples,1,1))    
   

    @partial(jax.jit, static_argnums=(0,))
    def ravel_gp(self, X, state):
        flat_samples = jnp.tile(X, (1,5,1))  # tile control points
        x,y,z = flat_samples.shape
        flat_samples = jnp.reshape(flat_samples, (x, y*z))   # flatten

        state = jnp.tile(state,(x,1)) # add state for prediction
        flat_samples = jnp.concatenate([state,flat_samples], axis=1)

        return flat_samples


    @partial(jax.jit, static_argnums=(0,)) 
    def sample(self, X,state): 
        flat_X = self.ravel_gp(X, state) # transform and add state for prediction

        predictive_dist = self.get_P(flat_X) # get GP-based CDF 

        target = X * predictive_dist # multiply distributions to get target
        mean = jnp.mean(target)
        stddev = jnp.std(target)

        key = jax.random.key(456)
        params_init = jnp.zeros((5,4))  

        @partial(jax.jit, static_argnums=(0,))
        def target_pdf(params):
            return jax.scipy.stats.multivariate_normal.logpdf(x=params, mean=mean, cov=stddev).sum()

        # sample from target with hmc
        chain = hmc.sample(key, params_init, target_pdf, n_steps = (self.burn_in + self.n_samples), n_leapfrog_steps=100, step_size=0.1) 

        return chain
    

class BNNSampling():
    def __init__(self, n_samples):
        total_samples = num_steps * 20
        all_samples = pd.read_csv("/home/ubuntu/sbmpc/sbmpc/datasets/dataset_2.data", header=None, delimiter=' ').values[:total_samples] # get dataset generated by dataset.py
        print(f"Retrieved dataset of {all_samples.shape[0]} samples with {all_samples.shape[1]} dimensions") # ((num_samples * num_steps), 114)

        X = all_samples[:, :-1]  
        y = all_samples[:, -1].reshape(-1, 1)

        Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.3, random_state=42) # split into training and test set

        self.Xtr = Xtr
        self.ytr = ytr
        self.Xte = Xte
        self.yte = yte

        log_ytr = np.log(ytr)
        log_yte = np.log(yte)

        y_scaler = StandardScaler().fit(log_ytr)
        scaled_ytr = y_scaler.transform(log_ytr)
        scaled_yte = y_scaler.transform(log_yte)

        x_scaler = StandardScaler().fit(Xtr)
        scaled_Xtr = x_scaler.transform(Xtr)
        scaled_Xte = x_scaler.transform(Xte)

        self.delta = 0.3
        # self.delta = 0.5
        self.burn_in = 10
        self.n_samples = n_samples

        def model(X, Y= None):
            w1 = numpyro.sample("w1", dist.Normal(0, 1).expand([X.shape[1], 10])) # TODO - look into more/better layers
            b1 = numpyro.sample("b1", dist.Normal(0, 1).expand([10]))

            w2 = numpyro.sample("w2", dist.Normal(0, 1).expand([10, 1]))
            b2 = numpyro.sample("b2", dist.Normal(0, 1))

            hidden = jnp.tanh(jnp.dot(X, w1) + b1)
            mean = jnp.dot(hidden, w2) + b2

            sigma = numpyro.sample("sigma", dist.Exponential(1.0))
            numpyro.sample("Y", dist.Normal(mean, sigma), obs=y)

        kernel = NUTS(model)
        mcmc = MCMC(kernel, num_warmup=100, num_samples=self.n_samples)
        # mcmc.run(jax.random.PRNGKey(0), X=X, Y=ytr)

        # self.posterior_samples = mcmc.get_samples()
        # self.pred_dist = Predictive(model, posterior_samples=self.posterior_samples,return_sites=["Y"])

        rng_key, rng_key_predict = jax.random.split(jax.random.PRNGKey(0))
        # helper function for HMC inference
        def run_inference(model, rng_key, X, Y):
            start = time.time()
            kernel = NUTS(model)
            mcmc = MCMC(
                kernel,
                num_warmup=100,
                num_samples=100,
                num_chains=1,
                progress_bar=False if "NUMPYRO_SPHINXBUILD" in os.environ else True,
            )
            mcmc.run(rng_key, X, y)
            # mcmc.print_summary()
            # print("\nMCMC elapsed time:", time.time() - start)
            return mcmc.get_samples()

        # helper function for prediction
        def predict(model, rng_key, samples, X):
            model = handlers.substitute(handlers.seed(model, rng_key), samples)
            # note that Y will be sampled in the model because we pass Y=None here
            model_trace = handlers.trace(model).get_trace(X=X, Y=None)
            return model_trace["Y"]["value"]
        
        samples = run_inference(model,  rng_key, X, y)
        # predict Y_test at inputs X_test
        vmap_args = (
            samples,
            jax.random.split(rng_key, 100 ),
        )
        predictions = jax.vmap(
            lambda samples, rng_key: predict(model, rng_key, samples, X)
        )(*vmap_args)
        predictions = predictions[..., 0]

        # # compute mean prediction and confidence interval around median
        mean_prediction = jnp.mean(predictions, axis=0)
        percentiles = np.percentile(predictions, [5.0, 95.0], axis=0)

        # # make plots
        # fig, ax = plt.subplots(figsize=(8, 6), constrained_layout=True)

        # # plot training data
        # ax.plot(Xte[:, 1], yte[:, 0], "kx")
        # # plot 90% confidence level of predictions
        # ax.fill_between(
        #     X[:, 1], percentiles[0, :], percentiles[1, :], color="lightblue"
        # )
        # # plot mean prediction
        # ax.plot(X[:, 1], mean_prediction, "blue", ls="solid", lw=2.0)
        # ax.set(xlabel="X", ylabel="Y", title="Mean predictions with 90% CI")

        # plt.savefig("bnn_plot.png")
        # Choose the feature index to visualize
        feature_idx = 1

        # Sort X and corresponding predictions by that feature
        sorted_idx = jnp.argsort(X[:, feature_idx])
        X_sorted = X[sorted_idx]
        mean_prediction_sorted = mean_prediction[sorted_idx]
        percentiles_sorted = percentiles[:, sorted_idx]

        # Plot
        fig, ax = plt.subplots(figsize=(8, 3), constrained_layout=True)

        # Plot training data
        ax.plot(Xte[:, feature_idx], yte[:, 0], "kx", label="Correct Outputs", color="green")

        # # Plot 90% confidence interval
        # ax.fill_between(
        #     X_sorted[:, feature_idx],
        #     percentiles_sorted[0, :],
        #     percentiles_sorted[1, :],
        #     color="lightblue",
        #     label="90% CI",
        # )

        # Plot mean prediction
        ax.plot(
            X_sorted[:, feature_idx],
            mean_prediction_sorted,
            color="blue",
            lw=2.0,
            label="Latent Distribution",
        )

        ax.set(xlabel=f"Input", ylabel="Output", title="BNN latent distribution")
        ax.legend()
        plt.savefig("bnn_plot.png")


    @partial(jax.jit, static_argnums=(0,))
    def get_P(self, X):
        predictions = self.pred_dist(rng_key=jax.random.PRNGKey(1), X=X)["Y"] 
        latent_dist = jnp.squeeze(predictions)

        mean = jnp.mean(latent_dist,axis=1)
        stddev = jnp.mean(latent_dist,axis=1)

        P = norm.cdf(self.delta, loc=mean, scale=stddev) # calc cdf for samples based on predictive mean and stddev, below a set threshold
        
        return np.reshape(P,(self.n_samples,1,1))
        

    @partial(jax.jit, static_argnums=(0,))
    def ravel_bnn(self, X, state):
        flat_samples = jnp.tile(X, (1,5,1))  # tile control points  - check that this is correct
        x,y,z = flat_samples.shape
        flat_samples = jnp.reshape(flat_samples, (x, y*z))   # flatten

        state = jnp.tile(state,(x,1)) # add state for prediction
        flat_samples = jnp.concatenate([state,flat_samples], axis=1)

        return flat_samples

    @partial(jax.jit, static_argnums=(0,))
    def sample(self, X, state):
        flat_X = self.ravel_bnn(X, state) # transform and add state for prediction

        predictive_dist = self.get_P(flat_X) # get GP-based CDF 

        target = X * predictive_dist # multiply distributions to get target
        
        mean = jnp.mean(target)  # too small
        stddev = jnp.std(target)

        key = jax.random.key(456)
        params_init = jnp.zeros((5,4))  # initialise control vars - TODO replace with better initial guess

        @partial(jax.jit, static_argnums=(0,))
        def target_pdf(params):
            return jax.scipy.stats.multivariate_normal.logpdf(x=params, mean=mean, cov=stddev).sum()

        # sample from target with hmc
        chain = hmc.sample(key, params_init, target_pdf, n_steps = (self.burn_in + self.n_samples), n_leapfrog_steps=100, step_size=0.1) # try different lengths of chains - consider burn-in
        return chain

# def plot_gp():
#     fig, ax = plt.subplots(figsize=(7.5, 2.5))
#     ax.plot(x, y, "x", label="Observations", color=cols[0], alpha=0.5)
#     ax.fill_between(
#         xtest.squeeze(),
#         predictive_mean - 2 * predictive_std,
#         predictive_mean + 2 * predictive_std,
#         alpha=0.2,
#         label="Two sigma",
#         color=cols[1],
#     )
#     ax.plot(
#         xtest,
#         predictive_mean - 2 * predictive_std,
#         linestyle="--",
#         linewidth=1,
#         color=cols[1],
#     )
#     ax.plot(
#         xtest,
#         predictive_mean + 2 * predictive_std,
#         linestyle="--",
#         linewidth=1,
#         color=cols[1],
#     )
#     ax.plot(
#         xtest, ytest, label="Latent function", color=cols[0], linestyle="--", linewidth=2
#     )
#     ax.plot(xtest, predictive_mean, label="Predictive mean", color=cols[1])
#     ax.legend(loc="center left", bbox_to_anchor=(0.975, 0.5))

if __name__ == '__main__':
    # GaussianProcessSampling(300)
    BNNSampling(300)
